# Fingerspelling Detection in American Sign Language

**Bowen Shi¹, Diane Brentari², Greg Shakhnarovich¹, Karen Livescu¹**
¹Toyota Technological Institute at Chicago, USA
²University of Chicago, USA
{bshi,greg, klivescu}@ttic.edu, dbrentari@uchicago.edu

---

An image displays a sequence of downsampled frames from a video of a person using American Sign Language. The overall English translation of the signed sentence is "Moving furtively, pirates steal the boy Patrick." Above the frames, labels identify the different signs being made. Non-fingerspelled signs are indicated with their glosses, such as "MOVE-FURTIVELY," "STEAL," and "BOY." The words that are fingerspelled, "PIRATES" and "PATRICK," are highlighted with open and close parentheses to indicate the start and end of the fingerspelling intervals. Below these words, canonical handshapes for each letter (P-I-R-A-T-E-S and P-A-T-R-I-C-K) are aligned with the corresponding video frames. This visual example serves to illustrate the two main goals discussed in the paper: first, the detection of the time intervals in the video where fingerspelling occurs, and second, the recognition (or transcription) of the sequence of letters within those detected intervals. The paper's focus is on the detection part, which is a necessary precursor to accurate recognition.

## Abstract

Fingerspelling, in which words are signed letter by letter, is an important component of American Sign Language. Most previous work on automatic fingerspelling recognition has assumed that the boundaries of fingerspelling regions in signing videos are known beforehand. In this paper, we consider the task of fingerspelling detection in raw, untrimmed sign language videos. This is an important step towards building real-world fingerspelling recognition systems. We propose a benchmark and a suite of evaluation metrics, some of which reflect the effect of detection on the downstream fingerspelling recognition task. In addition, we propose a new model that learns to detect fingerspelling via multi-task training, incorporating pose estimation and fingerspelling recognition (transcription) along with detection, and compare this model to several alternatives. The model outperforms all alternative approaches across all metrics, establishing a state of the art on the benchmark.

---

## 1. Introduction

Sign languages, such as American Sign Language (ASL), are natural languages expressed via movements of the hands, face, and upper body. Automatic processing of sign languages would assist communication between deaf and hearing individuals, but involves a number of challenges. There is no standard written form for sign languages. Automatic transcription of sign language into a written language such as English is in general a translation task. In addition, sign language gestures are often coarticulated and do not appear in their canonical forms.

In this paper, we focus on fingerspelling, a component of sign language in which words are signed letter by letter, with a distinct handshape or trajectory corresponding to each letter in the alphabet of a written language (e.g., the English alphabet for ASL fingerspelling). Fingerspelling is used for multiple purposes, including for words that do not have their own signs (such as many proper nouns, technical terms, and abbreviations) but also sometimes for emphasis or expediency. Fingerspelling accounts for 12% to 35% of ASL, where it is used more than in other sign languages. As important content words are commonly fingerspelled, automatic fingerspelling recognition can enable practical tasks such as search and retrieval in ASL media.

Compared to translation between a sign language and a written language, fingerspelling recognition involves transcription into a restricted set of symbols, with a monotonic alignment with the written form. Linguistically, fingerspelling is distinct from other elements of ASL, such as lexical signs and classifiers, so fingerspelling is likely to benefit from a separate model. The role that fingerspelling transcription is likely to play in ASL to English translation is similar to that of transliteration in written language translation. For all of these reasons, we believe that even as more general ASL processing methods are developed, it will continue to be beneficial to have dedicated fingerspelling detection and recognition modules.

Fingerspelling recognition has been widely studied. However, in most prior work, it is assumed that the input sequence contains fingerspelling only, sometimes extracted from longer sequences of signing via human annotation. Replacing human annotation with fully automatic detection of fingerspelling - identifying time spans in the video containing fingerspelling - is a hurdle that must be cleared to enable truly practical fingerspelling recognition "in the wild".

Fingerspelling detection has not been widely studied before. In principle it can be treated as a special case of action detection. However, in contrast to typical action detection scenarios, the actions in the fingerspelling "class" are highly heterogeneous and many fingerspelling handshapes are also used in non-fingerspelled signs. In addition, considering the goal of using the detector as part of a complete sign language processing system, a fingerspelling detector should be evaluated based on its effect on a downstream recognition model, a step not normally included in evaluation of action recognition. This makes common detection metrics, like average precision (AP) for action detection, less informative for fingerspelling detection.

Our design of a detection model is motivated by two observations. The first is that articulated pose, in particular handshape, plays a role in the distinctiveness of fingerspelling from other types of sign. At the same time, pose estimation, while increasingly successful in some domains, may be insufficiently accurate for directly informing fingerspelling recognition, as shown in previous work and in our experiments. Instead we incorporate pose estimation as part of training our model, but do not rely on explicit pose estimates at test time. The second observation concerns the goal of optimizing fingerspelling detection as a means to an end of improving downstream recognition. We address this by including a fingerspelling recognizer in model training. Our results show that this multi-task learning approach produces a superior detector compared to baselines that omit the pose and/or recognition losses.

Ours is to our knowledge the first work to demonstrate the effect of fingerspelling detection on fully automatic fingerspelling recognition in the wild. Beyond this novelty, our contributions are as follows. First, we propose an evaluation framework for fingerspelling detection that incorporates the downstream recognition task into the metrics, and introduce a benchmark based on extending a publicly available data set. Second, we investigate a number of approaches for fingerspelling detection, adapted from fingerspelling recognition and action detection, and develop a novel multi-task learning approach. Our model outperforms baseline detection approaches across all evaluation metrics, establishing a state of the art for the proposed benchmark.

---

## 2. Related Work

Early work on sign language recognition mostly focused on recognition of isolated signs. Recent work has increasingly focused on continuous sign language recognition, which transforms image sequences to glosses (for general sign language) or letter sequences (for fingerspelling), and translation. Approaches commonly separate between an image feature extraction component and a sequence modeling component. The former has moved from employing human-engineered features such as HoG to convolutional networks. Approaches for the sequence modeling component have included Hidden Markov Models (HMM), semi-Markov conditional random fields, connectionist temporal classification (CTC), and encoder-decoder models, which are largely borrowed from speech recognition.

Much of the prior work has been limited to data collected in a controlled environment. There has been a growing interest in sign language recognition "in the wild" (naturally occurring sign language media), which includes challenging visual conditions such as lighting variation, visual clutter, and motion blur, and often also more natural signing styles. Two recently released datasets of fingerspelling in the wild include data from 168 signers and tens of thousands of fingerspelling segments; these are the testbeds used in our experiments.

Prior work on fingerspelling detection employs visual features from optical flow or pre-defined hand keypoints. For sign language data collected in the wild, the quality of pose estimates is usually low, making them a poor choice as input to a detector (as we show in our experiments). Some researchers proposed a statistical procedure to detect changes in video for fingerspelling detection; however, these were tested anecdotally, in controlled environments and simplified scenarios.

Related work on detection of sign language segments (in video containing both signing and non-signing) uses recurrent neural networks (RNN) to classify individual frames into signing/non-signing categories. We compare our approach to baselines that use ideas from this prior work.

Prior work has largely treated the detection task as frame classification, evaluated via classification accuracy. Though sequence prediction is considered in some studies, the model evaluation is qualitative. In practice, a detection model is often intended to serve as one part of a larger system, producing candidate segments to a downstream recognizer. Frame-based metrics ignore the quality of the segments and their effect on a downstream recognizer.

Our modeling approach is based on the intuition that training with related tasks including recognition and pose estimation should help in detecting fingerspelling segments. Multi-task approaches have been studied for other related tasks. For example, some researchers jointly train an object detector and a word recognizer to perform text spotting. In contrast to this approach of treating detection and recognition as two parallel tasks, our approach further allows the detector to account for the performance of the recognizer. In sign language recognition, some work estimates human pose keypoints while training the recognizer. However, the keypoints used in that work are manually annotated. Here we study whether we can distill knowledge from an external imperfect pose estimation model (OpenPose).

---

## 3. Task and metrics

We are given a sign language video clip with N frames $I_{1},...,I_{N}$ containing n fingerspelling segments $\{x_{i}^{*}\}_{1\le i\le n}$ (* denotes ground truth), where $x_{i}^{*}=(s_{i}^{*},t_{i}^{*})$ and $s_{i}^{*},t_{i}^{*}$ are the start and end frame indices of the $i^{th}$ segment. The corresponding ground-truth letter sequences are $\{I_{i}^{*}\}_{1\le i\le n}$.

**Detection task** The task of fingerspelling detection is to find the fingerspelling segments within the clip. A detection model outputs m predicted segment-score pairs $\{(\hat{x}_{i},f_{i})\}_{1\le i\le m}$ where $\hat{x}_{i}$ and $f_{i}$ are the $i^{th}$ predicted segment and its confidence score, respectively.

**AP@IoU** A metric commonly used in object detection and action detection is AP@IoU. Predicted segments, sorted by score $f_{i}$, are sequentially matched to the ground-truth segment $x_{j}^{*}$ with the highest IoU (intersection over union, a measure of overlap) above a threshold $\delta_{IoU}$. Once $x_{j}^{*}$ is matched to a $\hat{x}_{i}$ it is removed from the candidate set. Let $k(i)$ be the index of the ground-truth segment $x_{k(i)}^{*}$ matched to $\hat{x}_{i}$; then formally,

$k(i) = \text{arg max}_{j:IoU(\hat{x}_{i}, x_{j}^{*}) > \delta_{IoU}, j \ne k(t) \forall t<i} IoU(\hat{x}_{i}, x_{j}^{*})$

Precision and recall are defined as the proportions of matched examples in the predicted and ground-truth segment sets, respectively. Varying the number of predictions m gives a precision-recall curve $p(\tilde{r})$. The average precision (AP) is defined as the mean precision over $N_{r}+1$ equally spaced recall levels $[0,1/N_{r},...,1]$, where the precision at a given recall is defined as the maximum precision at a recall exceeding that level: $AP= \frac{1}{N_{r}}\sum_{i=1}^{N_{r}}max_{\overline{r}\ge i/N_{r}}p(\overline{r}).$ AP@IoU can be reported for a range of values of $\delta_{IoU}$.

**Recognition task** The task of recognition is to transcribe $\hat{x}=(\hat{s},\hat{t})$ into a letter sequence $\tilde{I}.$ The recognition accuracy of a predicted sequence I w.r.t. the ground truth I is defined as $Acc(I^{*},\hat{I})=1-D(I^{*},\hat{I})/|I^{*}|$, where D is the edit (Levenshtein) distance and I is the length of a sequence. Note that Acc can be negative.

Prior work has considered recognition mainly applied to ground-truth segments $x^{*}$; in contrast, here we are concerned with detection for the purpose of recognition. We match a predicted $\hat{x}_{j}$ to a ground-truth $x_{i}^{*}$ and then evaluate the accuracy of $\hat{I}_{j}$ w.r.t. I. Thus, in contrast to a typical action detection scenario, here IoU may not be perfectly correlated with recognition accuracy. For example, a detected segment that is too short can hurt recognition much more than a segment that is too long. We propose a new metric, AP@Acc, to measure the performance of a fingerspelling detector in the context of a given downstream recognizer.

**AP@Acc** This metric uses the letter accuracy of a recognizer to match between predictions and ground-truth. It also requires an IoU threshold to prevent matches between non-overlapping segments. As in AP@IoU, predicted segments are sorted by score and sequentially matched:

$k(i)= \text{arg max}_{j: IoU(\hat{x}_{i}, x_{j}^{*}) > \delta_{IoU}, j \ne k(t) \forall t<i, Acc(I_{j}^{*},Rec(I_{\hat{s}_{i}:\hat{t}_{i}}))>\delta_{acc}} Acc(I_{j}^{*},Rec(I_{\hat{s}_{i}:\hat{t}_{i}}))$

where $Rec(I_{s:t})$ is the output (predicted letter sequence) from a recognizer given the frames $I_{s:t}$. We can report AP@Acc for multiple values of $\delta_{acc}$.

**Maximum Sequence Accuracy (MSA)** Both AP@IoU and AP@Acc measure the precision of a set of detector predictions. Our last metric directly measures just the performance of a given downstream recognizer when given the detector's predictions. We form the ground-truth letter sequence for the entire video $I_{1:N}$ by concatenating the letters of all ground-truth segments, with a special "no-letter" symbol separating consecutive letter sequences:

$L^{*}=\emptyset,I_{1}^{*},\emptyset,...,\emptyset,I_{n}^{*},\emptyset$.

Note that $\emptyset$ is inserted only where non-fingerspelling frames exist. For instance, the video in Figure 1 would yield $L^{*}=$ PIRATES$\emptyset$PATRICK. We similarly obtain a full-video letter sequence from the predicted segments. We suppress detections with score $f_{i}$ below $\delta_{f}$ and apply local non-maximum suppression, resulting in a set of non-overlaping segments $\hat{x}_{1},...,\hat{x}_{n}$. Each of these is fed to the recognizer, producing $\hat{I_{i}}=Rec(I_{\hat{s}_{i}:\hat{t}_{i}})$. Concatenating these in the same way gives us the full-video predicted letter sequence $\hat{L}(\delta_{f})$. We can now treat $L^{*}$ and L as two letter sequences, and compute the transcription accuracy. Maximum sequence accuracy (MSA) is defined as

$MSA=max_{\delta_{f}}Acc(L^{*},\hat{L}(\delta_{f}))$.

Like AP@Acc, MSA depends on both the detector and the given recognizer Rec. By comparing the MSA for a given detector and for an "oracle detector" that produces the ground-truth segments, we can obtain an indication of how far the detector output is from the ground-truth.

---

## 4. Models for fingerspelling detection

A diagram illustrates three different model architectures for fingerspelling detection.
**(a) Baselines 1 and 2:** This model starts with video frames, which are fed into a Convolutional network (Conv). The output features are then passed to a Bidirectional LSTM (Bi-LSTM). For Baseline 1, the Bi-LSTM produces frame labels (binary classification) which are trained with a per-frame cross-entropy loss, $L_{frame-seq}$. For Baseline 2, it uses augmented letter labels and a modified CTC loss. From these frame labels, segment proposals are derived.
**(b) Baseline 3:** This model, a modified R-C3D, also starts with video frames passing through a Conv layer. The resulting features are processed by another Conv layer to generate segment proposals directly. These proposals are trained using a standard detection loss, $L_{det}$.
**(c) Our model (multi-task):** This is the most complex architecture. Video frames are processed by a Conv layer. The features are then used in multiple parallel tasks. One path leads to another Conv layer to generate segment proposals, which are trained with a detection loss, $L_{det}$. A second path feeds the features into a POSE estimation module, which generates pose pseudo-labels and is trained with a pose loss, $L_{pose}$. The segment proposals are also used by a recognition (REC) module, which outputs a letter sequence (e.g., H-E-L-L-O) and is trained with a recognition loss, $L_{rec}$. The detections are also fed into a second stage REC module, trained with a letter error loss, $L_{ler}$. The model is trained with a combination of all these losses: detection, recognition, letter error, and pose estimation. A Region of Interest (ROI) is indicated for a second stage of processing.

### 4.1. Baseline models

**Baseline 1: Frame-based detector** This model classifies every frame as positive (fingerspelling) or negative (non-fingerspelling), and is trained using the per-frame cross-entropy loss, weighted to control for class imbalance. Each frame is passed through a convolutional network to extract visual features, which are then passed to a multi-layer bidirectional long short-term memory RNN (LSTM) to take temporal context into account, followed by a linear/sigmoid layer producing per-frame class posterior probabilities. To convert the frame-level outputs to predicted segments, we first assign hard binary frame labels by thresholding the posteriors by $\overline{p}.$ Contiguous sequences of positive labels are taken to be the predicted segments, with a segment score f computed as the average posterior of the fingerspelling class for its constituent frames. We repeat this process for decreasing values of p, producing a pool of (possibly overlapping) segments. Finally, these are culled using thresholding on f and non-maximum suppression limiting overlap, producing a set of $\hat{x}_{S}$ as the final output.

**Baseline 2: Recognition-based detector** Instead of classifying each frame into two classes, this baseline addresses the task as a sequence prediction problem, where the target is the concatenation of true letter sequences separated by $\emptyset$ indicating non-fingerspelling segments, as in (3). The frame-to-letter alignment in fingerspelling spans is usually unknown during training, so we base the model on connectionist temporal classification (CTC): We generate frame-level label softmax posteriors over possible letters, augmented by $\emptyset$ and a special blank symbol, and train by maximizing the marginal log probability of label sequences that produce the true sequence under CTC's "label collapsing function" that removes duplicate frame labels and then blanks (we refer to this as the CTC loss). In this case we have a partial alignment of sequences and frame labels, since we know the boundaries between $\emptyset$ and fingerspelling segments, and we use this information to explicitly compute a frame-level log-loss in non-fingerspelling regions. This modification stabilizes training and improves performance. At test time, we use the model's per-frame posterior probability of fingerspelling, $1-p(\emptyset)$, and follow the same process as in baseline 1 (frame-based) to convert the per-frame probabilities to span predictions.

**Baseline 3: Region-based detector** This model directly predicts variable-length temporal segments that potentially contain fingerspelling, and is adapted from R-C3D, a 3D version of the Faster-RCNN. The model first applies a 2D ConvNet on each frame; additional 3D convolutional layers are applied on the whole feature tensor to capture temporal information. Unlike prior work, we did not directly apply an off-the-shelf 3D ConvNet such as C3D, since its large stride may harm our ability to capture the delicate movements and often short sequences in fingerspelling. A region proposal network is applied to the feature tensor, predicting shifts of potential fingerspelling segments with respect to (temporal, ID) anchors, and a binary label indicating whether the predicted proposal contains fingerspelling. The detector is trained with a loss composed of two terms for (binary) classification and (positional) regression, $L_{det}=L_{cls}+L_{req}$. See prior work for further details. At test time, we use greedy non-maximum suppression (NMS) on fingerspelling proposals to eliminate highly overlapping and low-confidence intervals. Since fingerspelling detection is a binary classification task, we do not use a second stage classification subnet as in the original RC3D.

### 4.2. Our approach: A multi-task model

Our approach is based on a region-based detector, but with the key difference that fingerspelling recognition and pose estimation are integrated into the model's training. Recognition loss is calculated by feeding the fingerspelling segments to a recognizer. The idea behind this is that including the recognition task might help the model learn richer features for fingerspelling, which could improve its ability to differentiate between fingerspelling and non-fingerspelling. The recognizer in this context functions similarly to the classification sub-network in RC3D. However, since we don't assume frame-letter alignment is available during training, we directly build a sub-network for letter sequence prediction (the orange "REC" in Figure 2). This recognition sub-network is based on the attention-based model proposed for fingerspelling recognition in [46], utilizing only the ground-truth segment for the recognition loss. The recognition loss $L_{rec}$ is computed as the CTC loss summed over the proposed regions predicted by the detector:

$L_{rec}=\sum_{i=1}^{n}L_{ctc}(Rec(I_{s_{i}^{*};t_{i}^{*}}),I_{i}^{*})$ (5)

where n represents the number of true fingerspelling segments.

Letter error rate loss While $L_{rec}$ should help in learning an improved image feature space, the detector's performance doesn't directly impact the recognizer because $L_{rec}$ only uses ground-truth segments. $L_{det}$ encourages the model to produce segments that are spatially close to the ground truth. What's missing is the objective of making the detector perform well for downstream recognition. To address this, we add a loss that measures the letter error rate of a recognizer applied to proposals from the detector:

$L_{ler}=-\sum_{i=1}^{m}p(\hat{x}_{i})Acc(I_{k(i)}^{*},Rec(I_{\hat{s}_{i}:\hat{t}_{i}})),$ (6)

where, as before, $k(i)$ is the index of the ground-truth segment matched (based on IoU) to the $z^{th}$ proposal $\hat{x}_{i}=(s_{i},t_{i})$, $m$ is the number of proposals output by the detector, and $Acc$ is the recognizer accuracy. This loss can be interpreted as the expectation of the negative letter accuracy of the recognizer on the proposals provided by the detector. Since $Acc$, which depends on the edit distance, is non-differentiable, we approximate the gradient of (6) as in REINFORCE [52]:

$L_{ler} = \frac{1}{M} \sum_{i=1}^{M} Acc(I_{k(i)}^*, Rec(I_{\hat{s}_i:\hat{t}_i})) \log p(\hat{x}_i)$ (7)

where the sum is over the M highest scoring proposals, and $p(\hat{x}_{i})$ is the normalized score $f_{i}/\sum_{i=1}^{M}f_{i}$.

Pose estimation loss Given that sign language largely relies on body articulation, incorporating pose into the model is a natural consideration. However, we cannot assume access to ground-truth pose even in the training data. Instead, we can rely on general-purpose human pose estimation models, such as OpenPose [6], which are trained on large sets of annotated images.

Previous work on sign language has utilized pose estimates extracted from an external model as input for sign language-related tasks [24, 38, 41, 32, 8]. In those studies, which used controlled studio data, the quality of extracted keypoints was reliable. However, on more challenging real-world data like ChicagoFSWild/ChicagoFSWild+ [46], we observe that the detected keypoints from OpenPose are considerably less reliable, likely due in part to widespread motion blur in the data (refer to Figure 5 for examples). Indeed, our experiments show that using automatically extracted poses as input to the model does not significantly improve performance. Instead of relying on estimated pose during testing, we treat the estimated pose as an additional source of supervision for our model during training. We use keypoints from OpenPose as pseudo-labels to help distill knowledge from the pre-trained pose model into our detection model. As pose is used only as an auxiliary task, the quality of these pseudo-labels has less impact on detection performance than using pose as input at test time. The pose estimation sub-network takes the feature maps extracted by the model (which are shared with the detector and recognizer) and applies several transposed convolutional layers to increase spatial resolution, producing for frame $I_{t}$ a set of heat maps $b_{t,1},...,b_{t,P}$ for P keypoints in the OpenPose model. We also use OpenPose to extract keypoints from $I_{t}$; each estimated keypoint p is accompanied by a confidence score $\sigma_{t,p.}$ We convert these estimates to pseudo-ground truth heatmaps $b_{t,1}^{*},...,b_{t,P}^{*}.$ The pose loss is the per-pixel Euclidean distance between the predicted and pseudo-ground truth maps:

$L_{pose}=\sum_{t=1}^{T}\sum_{p=1}^{P}||b_{t,p}-b_{t,p}^{*}||^{2}\cdot1_{\sigma_{t,p}>\tau},$ (8)

where the threshold on the OpenPose confidence is used to ignore low-confidence pseudo-labels.

Combining all components, the loss for training our model is:

$L=L_{det}+\lambda_{rec}L_{rec}+\lambda_{ler}L_{ler}+\lambda_{pose}L_{pose},$ (9)

with tuned weights $\lambda_{ler}$, $\lambda_{rec}$, $\lambda_{pose}$ controlling the relative importance of different loss components.

Second stage refinement We anticipate that reasoning about fine-grained motion and handshape differences will be helpful, so we aim to use high-resolution input images. However, since the input video clip covers hundreds of frames, the images need to be downsampled to fit into the memory of a typical GPU.

To mitigate the issue of low resolution in local regions, we "zoom in" on the original image frames using the attention map produced by the recognizer sub-network. This approach is based on the iterative attention method described in [46]. Large attention values indicate higher importance of the corresponding region for fingerspelling; therefore, cropping the ROI surrounding a location with high attention values helps increase the resolution of that part while avoiding irrelevant areas. To achieve this, we first complete the training of the entire model described above. Suppose the image sequence of original resolution is $I_{1:N}^{o}$, the lower-resolution frame sequence used in the first round of training is $I_{1:N}^{g}$, and the trained model is H. We perform inference on $I_{1:N}^{g}$ with the recognition sub-network $\mathcal{H}_{rec}$ to produce a sequence of attention maps $A_{1:N}$. We use $A_{1:N}$ to crop $I_{1:N}^{o}$ into a sequence of local ROIs $I_{1\cdot N}^{l}$. Specifically, at timestep n, we place a bounding box $b_{n}$ of size $R|I_{n}|$ centered on the peak of attention map $A_{n}$, where R is the zooming factor. We average the bounding boxes of the $2a+1$ frames centered on the $n^{th}$ frame to produce a smoother cropping "tube" $b_{1:N}^{s}$:

$b_{n}^{s}=\frac{1}{2a+1}\sum_{i=-a}^{a}(b_{n+i})$ (10)

The local ROIs $I_{1:N}^{t}$ are cropped from $I_{1:N}^{o}$ with $b_{1:N}^{s}$. Finally, we perform a second stage of training with both $I_{1:N}^{g}$ and $I_{1:N}^{l}$ as input. At each timestep n, the backbone convolutional layers are applied to $I_{n}^{g}$ and $I_{n}^{l}$ to obtain global and local feature maps, respectively. These two feature maps are concatenated for detection and recognition. The pseudo-ground truth pose maps are estimated on $I_{t}^{(g)}$ and $\tilde{I}_{t}^{(l)}$ separately. The overall loss for the second-stage training is:

$L^{\mathfrak{final}=L_{det}+\lambda_{rec}L_{rec}+\lambda_{ler}L_{ler}+\lambda_{pose}(L_{pose}^{(g)}+L_{pose}^{(l)})$ (11)

The key difference between our approach and the iterative attention of [46] is that we do not discard the input images; instead, we use the newly generated ROIs as extra input. In fingerspelling detection, both global context (e.g., upper body position) and local details (e.g., handshape) are important.

5. Experiments

5.1. Setup

We conduct experiments on ChicagoFSWild [47] and ChicagoFSWild+ [46], two large-scale ASL fingerspelling datasets collected in the wild. Although these datasets were introduced for fingerspelling recognition (with the boundaries given), the URLs of the raw ASL videos and the fingerspelling start/end timestamps are provided. We split each video clip into 300-frame chunks (~12s) with a 75-frame overlap between chunks. The longest fingerspelling sequence in the data is 290 frames long. We use the same training/dev/test data split as in the original datasets (see additional data statistics in the supplementary material). The image frames are center-cropped and resized to $108\times108$.

We use the convolutional layers from VGG-19 [48] pre-trained on ImageNet [10] as our backbone network, and fine-tune the weights during training. For baselines 1 and 2, an average pooling layer is applied to the feature map, yielding a 512-dimensional vector for each frame, which is then fed into a one-layer Bi-LSTM with 512 hidden units. In baseline 3 and our model, the feature map is further passed through a 3D conv + maxpooling layer (with temporal stride 8). In the region proposal network, the lengths of the anchors are fixed at 12 values ranging from 8 to 320, chosen according to the typical lengths of fingerspelling sequences in the data. The IoU thresholds for positive/negative anchors are 0.7/0.3, respectively. The predicted segments are refined with NMS at a threshold of 0.7. The magnitude of optical flow is used as a prior attention map as in [46]. We use the same recognizer (Rec) for (5) and (6). The pose sub-network is composed of one transposed convolutional layer with stride 2. We use OpenPose [6] to extract 15 body keypoints and $2\times21$ hand keypoints (both hands) as pseudo pose labels. Keypoints with confidence below $\tau=0.5$ are dropped. For second-stage refinement, the moving averaging of bounding boxes in (10) uses 11 frames $(a=5)$ and the frame sequence is downsampled by a factor of 2 to save memory. The loss weights ($\lambda$s) are tuned on the dev set. To evaluate with AP@Acc and MSA, we train a reference recognizer with the same architecture as the recognition-based detector on the fingerspelling training data. The recognizer achieves an accuracy (%) of 44.0/62.2 on ground-truth fingerspelling segments on ChicagoFSWild/ChicagoFSWild+. The accuracy is slightly worse than that of [46] because we used a simpler recognizer. In particular, we skipped the iterative training in [46], used lower-resolution input, and did not use a language model. We consider $\delta_{IoU}\in\{0.1,0.3,0.5\}$ and $\delta_{acc}\in\{0,0.2,0.4\}$; $\delta_{IoU}$ is fixed at 0 for AP@Acc.

5.2. Results

Main results Table 1 compares models using the three proposed metrics. The values of AP@Acc and MSA depend on the reference recognizer. For ease of comparison, we also show the oracle results when the same recognizer is given the ground-truth fingerspelling segments. Overall, the relative model performance is consistent across metrics. Methods that combine detection and recognition outperform those that do purely detection (baseline 2 vs. 1, our model vs. baseline 3). In addition, region-based methods (baseline 3 and our model) outperform frame-based methods (baseline 1 & 2), whose segment predictions lack smoothness. We can see this also by examining the frame-level performance of the three baselines. Their frame-level average precisions are 0.522 (baseline 1), 0.534 (baseline 2), and 0.588 (baseline 3), which are much closer than the segment-based metrics, showing how frame-level metrics can obscure important differences between models. More details on precision and recall can be found in the supplementary material. The trends in results are similar on the two datasets. For the remaining analysis we report results on the ChicagoFSWild dev set. In addition to baselines adapted from related tasks, Table 1 includes a comparison to a recent method developed for action detection: the boundary matching network [35]. We also compared to the multi-stage temporal convolutional network [13] (details in the supplementary). Neither approach is better than ours on the fingerspelling benchmark.

The chart, titled "Sequence Accuracy (%)" on the y-axis and "Score Threshold" on the x-axis, plots the performance of four different models: Base 1, Base 2, Base 3, and Our Model. The lines represent the sequence accuracy at various score thresholds ranging from 0.4 to 1.0. Our Model consistently achieves the highest sequence accuracy across all thresholds, peaking around 0.99 threshold. Base 3 also shows strong performance, generally second best, while Base 1 and Base 2 lag behind, with Base 2 performing slightly better than Base 1 at higher thresholds.

Table 1 provides a comparison of model performance on the ChicagoFSWild and ChicagoFSWild+ test sets. It presents metrics such as AP@IoU (at 0.1, 0.3, and 0.5 IoU thresholds), AP@Acc (at 0.0, 0.2, and 0.4 accuracy thresholds), and MSA. For each dataset, the table includes results for Baseline 1, Baseline 2, Baseline 3, BMN (Boundary Matching Network), Ours (Our Model), and GT (Ground Truth).

On the ChicagoFSWild dataset:
- **AP@IoU (0.1, 0.3, 0.5):** "Ours" consistently shows the highest performance among the non-GT models (e.g., AP@0.1 for Ours is 0.442, compared to 0.310 for Base 2 and 0.447 for Base 3). GT naturally shows 1.00 for all IoU thresholds.
- **AP@Acc (0.0, 0.2, 0.4):** "Ours" also generally leads here (e.g., AP@0.0 for Ours is 0.249, higher than Base 1's 0.062, Base 2's 0.158, and Base 3's 0.216).
- **MSA:** "Ours" achieves 0.452, outperforming Base 1 (0.231), Base 2 (0.256), and Base 3 (0.320).

On the ChicagoFSWild+ dataset:
- **AP@IoU (0.1, 0.3, 0.5):** Similar to ChicagoFSWild, "Ours" generally shows the best performance (e.g., AP@0.1 for Ours is 0.590, compared to 0.443 for Base 2 and 0.560 for Base 3). GT again shows 1.00.
- **AP@Acc (0.0, 0.2, 0.4):** "Ours" performs strongly (e.g., AP@0.0 for Ours is 0.433, higher than Base 1's 0.211, Base 2's 0.323, and Base 3's 0.426).
- **MSA:** "Ours" achieves 0.503, outperforming Base 1 (0.267), Base 2 (0.390), and Base 3 (0.477).

Overall, the table demonstrates that "Our Model" consistently outperforms the baselines and BMN across various metrics on both datasets, though it does not reach the oracle performance of the ground-truth (GT) segments.

Analysis of evaluation metrics The AP@Acc results are largely invariant to $\delta_{IoU}$ (see supplementary material) and we report results for $\delta_{IoU}=0$ (i.e., matching ground truth segment to detections based on accuracy, subject to non-zero overlap). We also examine the relationship between sequence accuracy and the score threshold of each model (Figure 3). Our model achieves higher sequence accuracy across all thresholds. The threshold producing the best accuracy varies for each model. The average IoUs of the four models for the optimal threshold $\delta_{f}$ are 0.096, 0.270, 0.485, and 0.524 respectively.

Ablation study Our model reduces to baseline 3 when all loss terms except the detection loss are removed. Table 2

For the region-based model (baseline 3) we take the maximum probability of all proposals containing a given frame as that frame's probability.

Figure 3 is a line graph titled "Sequence Accuracy (%)" versus "Score Threshold." The x-axis represents the "Score Threshold" from 0.4 to 1.0, and the y-axis represents "Sequence Accuracy (%)" from 0 to 50. Four lines, representing "Base 1", "Base 2", "Base 3", and "Our Model", illustrate how the sequence accuracy of each model changes with different score thresholds. "Our Model" consistently shows the highest sequence accuracy across all thresholds, reaching nearly 40% at higher thresholds. "Base 3" performs second best, while "Base 1" and "Base 2" show lower accuracy, with "Base 2" generally outperforming "Base 1" for most thresholds. This figure demonstrates the dependence of sequence accuracy on the score threshold for each model.

shows how model performance improves as more tasks are added. The gain due to the recognition loss alone is smaller than for the frame-based models (base 1 vs. base 2). The recognition sub-network contributes more through the LER loss, which communicates the recognition error to the detector directly. The second-stage refinement does not always boost AP@IoU; the gain is more consistent in the accuracy-based metrics (AP@Acc, MSA). The zoom-in effect of the second-stage refinement increases the resolution of the hand region and improves recognition, though the IoU may remain similar. The downsampling in the second-stage refinement also leads to some positioning error, reflected in the slight drop in AP at $IoU=0.5$ though a minor temporal shift does not always hurt recognition.

Examples Figure 4 shows examples in which our model correctly detects fingerspelling segments while baseline 3 fails. Fingerspelling can include handshapes that are visually similar to non-fingerspelled signs (Figure 4a). Fingerspelling recognition, as an auxiliary task, may improve detection by helping the model distinguish among fine-grained handshapes. The signer's pose may provide additional information for detecting fingerspelling (Figure 4b). Figure 4c shows an example where the signing hand is a small portion of the whole image; baseline 3 likely fails due to the low resolution of the signing hand. The second-stage

Figure 4 displays example segments detected by different models, specifically comparing "Base 3" with "Our Model" (with Ground Truth, GT, for reference). Each row (a, b, c) presents a sequence of five images. The bottom row for each example shows the Regions of Interest (ROIs) used in the second-stage refinement, and the full sequence is downsampled.

In row (a), the images depict a woman fingerspelling. "Base 3" shows incorrect or missed detections, while "Our Model" accurately captures the fingerspelling sequence. This case highlights how fingerspelling can involve handshapes visually similar to non-fingerspelled signs, and the benefit of our model in distinguishing them.

In row (b), the images feature a man fingerspelling. "Base 3" again shows detection failures, whereas "Our Model" successfully identifies the fingerspelled segment, suggesting that incorporating the signer's pose can provide crucial additional information for detection.

In row (c), the images show a man whose signing hand occupies a small portion of the overall frame. "Base 3" likely fails due to the low resolution of the signing hand in its detection, while "Our Model" with its second-stage refinement (as indicated by the ROIs in the bottom row), accesses a higher-resolution ROI of the hand, leading to correct detection.

Table 2 illustrates the impact of adding different loss components during our model's training, with results on the ChicagoFSWild development set. The baseline is "Base 3", and subsequent columns show the incremental improvements by adding: "+ rec" (recognition loss), "+LER" (Letter Error Rate loss), "+pose" (pose estimation loss), and "+2nd stage" (second-stage refinement).

The table includes metrics for AP@IoU (at 0.1, 0.3, and 0.5 IoU thresholds), AP@Acc (at 0.0, 0.2, and 0.4 accuracy thresholds), and MSA.

- **AP@IoU:**
    - Adding "+ rec" to Base 3 slightly decreases AP@0.1 and AP@0.3 but increases AP@0.5.
    - Adding "+LER" generally improves AP@IoU values across all thresholds compared to "+ rec".
    - Adding "+pose" further increases AP@IoU values.
    - Adding "+2nd stage" shows a slight increase in AP@0.1 and AP@0.3, but a minor drop in AP@0.5, as noted in the text about downsampling affecting positioning error.
- **AP@Acc and MSA:**
    - There is a consistent upward trend in AP@Acc values and MSA as each loss component is added. This indicates that recognition-based metrics benefit more consistently from the added tasks. For example, MSA increases from 0.359 (Base 3 + rec) to 0.386 (+2nd stage).

The table supports the claim that each additional task contributes to the overall improvement in model performance, particularly for accuracy-based metrics.

Table 3 presents a comparison among different modalities for the region-based detector (baseline 3) on the ChicagoFSWild dev set. The table evaluates performance based on AP@IoU (at 0.1, 0.3, and 0.5 IoU thresholds) across various input configurations: "RGB Pose Opt" (RGB, Pose, and Optical Flow inputs combined), "RGB+Opt (in)" (RGB and Optical Flow as input), "RGB+Pose (in)" (RGB and Pose as input), and "RGB+Pose (out)" (Our model, where pose estimation is a secondary task, not directly an input).

- **AP@IoU (0.1, 0.3, 0.5):**
    - "RGB Pose Opt" (combining all three as input) generally shows lower performance compared to the other configurations, especially RGB+Opt (in) and RGB+Pose (out).
    - "RGB+Opt (in)" consistently performs well, showing better AP@IoU values than "RGB+Pose (in)" and the baseline "RGB Pose Opt". For instance, AP@0.1 is 0.503.
    - "RGB+Pose (in)" shows slightly lower performance than "RGB+Opt (in)".
    - "RGB+Pose (out)" (Our model) demonstrates the best overall performance among these configurations. For example, AP@0.1 is 0.505, AP@0.3 is 0.478, and AP@0.5 is 0.366, indicating that treating pose estimation as a secondary task (distilling knowledge) is more effective than using it directly as an input modality.

This table highlights that while motion (optical flow) can be helpful as an input modality, our approach of incorporating pose estimation as an auxiliary task (RGB+Pose (out)) generally yields superior detection performance compared to directly inputting pose or combining all modalities.

refinement enables the model to access a higher-resolution ROI, leading to a correct detection.

Error analysis Qualitatively, we notice three common sources of false positives: (a) near-face sign, (b) numbers, and (c) fingerspelling handshape used in regular signing (see Figure 6). Such errors can potentially be reduced by incorporating linguistic knowledge in the detector, which is left as future work. We also observe that short fingerspelling segments are harder to detect. See the supplementary material for the performance breakdown according to length.

Other input modalities Our model is trained on RGB images. Motion and pose are two common modalities used in sign language tasks [24, 38, 55], so it is natural to ask whether they would be helpful instead of or in addition to RGB input. We use magnitude of optical flow [14] as the motion image and the pose heatmap from OpenPose as the pose image. Raw RGB frames as input outperform the other

Figure 5 shows two examples of pose estimation failure cases, illustrating the challenges of accurately estimating pose in real-world sign language data.
- **Example (a):** A sequence of images featuring a bald man. The pose estimation appears to fail in capturing the hand or arm movements accurately, possibly due to blur or difficult lighting conditions.
- **Example (b):** A sequence of images featuring a woman. Similar to example (a), the pose estimation seems unreliable, with keypoints potentially being missed or misplaced, especially around the hands, which are crucial for fingerspelling.
These images support the claim that OpenPose can struggle with detecting keypoints in challenging real-world scenarios, particularly for the signing hand, impacting the reliability of pose as a direct input.

Figure 6 presents examples of false positive detections made by the model. The images illustrate common scenarios where the model incorrectly identifies non-fingerspelling segments as fingerspelling.
- **Image (a):** Shows a signer with a handshape near their face. The gloss is "PEOPLE," indicating a regular sign, not fingerspelling, yet it was falsely detected.
- **Image (b):** Displays a signer forming what appears to be a number. The gloss is "2018," which involves numbers, often visually similar to fingerspelling, leading to a false positive.
- **Image (c):** Features a signer using a fingerspelling-like handshape within a regular sign. The gloss is "THAT-[C..}," suggesting that the handshape is part of a broader sign, not an isolated fingerspelled word.
These examples highlight specific challenges for the detector, particularly in distinguishing subtle handshape differences and contextual cues that differentiate true fingerspelling from similar non-fingerspelling signs, numbers, or near-face signs.

two modalities, although each of them can be slightly helpful when concatenated with the RGB input (see Table 3). The pose image is less consistently helpful, likely because pose estimation is very challenging on our data and OpenPose often fails to detect the keypoints especially in the signing hand (see Figure 5). However, treating pose estimation as a secondary task, as is done in our model, successfully "distills" the pose model's knowledge and outperforms the use of additional modalities as input. We note that using all three modalities can boost performance further. Using both RGB and motion images as input while jointly estimating pose, the detector achieves .523/.495/.367 for $AP\oplus IoU(0.1/0.3/0.5)$ improving over the best model in Table 3. However, optimizing performance with multiple modalities is not our main focus, and we leave further study of this direction to future work.

6. Conclusion

We study the task of sign language fingerspelling detection, motivated by the goal of developing it as an upstream component in an automatic recognition system. We propose a benchmark, based on extending a previously released fingerspelling recognition dataset, and establish a suite of metrics to evaluate detection models both on their own merits and on their contribution to downstream recognition. We investigate approaches adapted from frame classification, fingerspelling recognition and action detection, and demonstrate that a novel, multi-task model achieves the best results across metrics. Beyond standard detection loss, this model incorporates losses derived from recognition and pose estimation; we show that each of these contributes to the superior performance of the model. Our results provide, for the first time, a practical recipe for fully automatic detection and recognition of fingerspelling in real-world ASL media. While our focus has been on fingerspelling, we expect that the techniques, both in training and in evaluation of the methods, will be helpful in the more general sign language detection and recognition domain, which remains our goal for future work.
